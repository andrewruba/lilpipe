{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# tinypipe Demo Notebook\n",
        "\n",
        "This notebook demonstrates the key features of **tinypipe**, a lightweight, Pydantic-powered library for sequential pipelines in Python. tinypipe is ideal for data processing, scientific workflows (e.g., ligand-binding assays), and any task needing a clear, linear sequence of steps.\n",
        "\n",
        "## Features Demonstrated\n",
        "- **Sequential Workflows**: Run steps in a fixed order.\n",
        "- **Decorator API**: Define steps with `@pipestep` for simplicity.\n",
        "- **Pydantic Context**: Type-safe state management with `PipelineContext`.\n",
        "- **Smart Caching**: Skip unchanged steps using `fingerprint_keys`.\n",
        "- **Flexible Control**: Use `ctx.abort_pass()` or `ctx.abort_pipeline()`.\n",
        "- **Composable Steps**: Nest workflow steps.\n",
        "- **Bio-Inspired Workflow**: Example for ligand-binding assay (LBA) processing.\n",
        "- **Observability**: Inspect timing and cache hits via `step_meta`.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import tinypipe components\n",
        "from tinypipe import Step, pipestep, Pipeline, PipelineContext\n",
        "import logging\n",
        "\n",
        "# Configure logging to see pipeline execution\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Sequential Workflows\n",
        "\n",
        "tinypipe runs steps in a fixed order within a `Pipeline`. Here, we create a simple pipeline with three steps to process data sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@pipestep(name=\"load_data\")\n",
        "def load_data(ctx: PipelineContext) -> PipelineContext:\n",
        "    ctx.data = [1.0, 2.0, 3.0]  # Simulated input data\n",
        "    return ctx\n",
        "\n",
        "\n",
        "@pipestep(name=\"double\")\n",
        "def double(ctx: PipelineContext) -> PipelineContext:\n",
        "    ctx.data = [x * 2 for x in ctx.data]\n",
        "    return ctx\n",
        "\n",
        "\n",
        "@pipestep(name=\"offset\")\n",
        "def offset(ctx: PipelineContext) -> PipelineContext:\n",
        "    ctx.data = [x + 1 for x in ctx.data]\n",
        "    return ctx\n",
        "\n",
        "\n",
        "# Create and run the pipeline\n",
        "pipeline = Pipeline([load_data, double, offset], name=\"simple_pipeline\")\n",
        "ctx = PipelineContext()\n",
        "pipeline.run(ctx)\n",
        "\n",
        "print(f\"Result: {ctx.data}\")  # Expected: [3.0, 5.0, 7.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Decorator API and Pydantic Context\n",
        "\n",
        "The `@pipestep` decorator simplifies step definitions, while `PipelineContext` (Pydantic-based) ensures type-safe state with flexible attributes (`extra=\"allow\"`). Here, we use `@pipestep` to clean data and store results in `ctx.output`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@pipestep(name=\"clean_data\", fingerprint_keys=(\"input\",))\n",
        "def clean_data(ctx: PipelineContext) -> PipelineContext:\n",
        "    ctx.output = [x for x in ctx.input if x is not None]\n",
        "    return ctx\n",
        "\n",
        "\n",
        "# Run a single step\n",
        "ctx = PipelineContext(input=[1, None, 3, None, 5])\n",
        "clean_data.run(ctx)\n",
        "print(f\"Cleaned data: {ctx.output}\")  # Expected: [1, 3, 5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Smart Caching\n",
        "\n",
        "tinypipe uses `fingerprint_keys` to cache step results based on input state. If inputs haven't changed, the step is skipped (cache hit). Let's run the `clean_data` step again with the same input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run twice with same input\n",
        "ctx = PipelineContext(input=[1, None, 3, None, 5])\n",
        "clean_data.run(ctx)\n",
        "print(f\"Cleaned data: {ctx.output}\")  # Expected: [1, 3, 5]\n",
        "clean_data.run(ctx)  # Should log \"Skipping clean_data (cache hit)\"\n",
        "print(f\"Cleaned data (cached): {ctx.output}\")\n",
        "\n",
        "# Change input to bypass cache\n",
        "ctx = PipelineContext(input=[1, None, 4, None, 5])\n",
        "clean_data.run(ctx)  # Should recompute\n",
        "print(f\"Cleaned data (new input): {ctx.output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Flexible Control with Pipeline Signals\n",
        "\n",
        "Use the context helpers:\n",
        "- `ctx.abort_pass()` to stop the current pass and let the `Pipeline` run another (up to `max_passes`).\n",
        "- `ctx.abort_pipeline()` to stop everything immediately.\n",
        "\n",
        "Here, we demonstrate retrying a pipeline if data is invalid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@pipestep(name=\"load_with_error\")\n",
        "def load_with_error(ctx: PipelineContext) -> PipelineContext:\n",
        "    ctx.attempt = getattr(ctx, \"attempt\", 0) + 1\n",
        "    if ctx.attempt == 1:\n",
        "        ctx.data = [-1.0, 2.0, 3.0]  # Invalid data (negative value)\n",
        "    else:\n",
        "        ctx.data = [1.0, 2.0, 3.0]  # Fixed data\n",
        "    return ctx\n",
        "\n",
        "\n",
        "@pipestep(name=\"validate\", fingerprint_keys=(\"data\",))\n",
        "def validate(ctx: PipelineContext) -> PipelineContext:\n",
        "    if any(x < 0 for x in ctx.data):\n",
        "        ctx.abort_pass()  # Retry another pass\n",
        "    return ctx\n",
        "\n",
        "\n",
        "pipeline = Pipeline([load_with_error, validate], name=\"retry_pipeline\", max_passes=3)\n",
        "ctx = PipelineContext()\n",
        "pipeline.run(ctx)\n",
        "\n",
        "print(f\"Final data: {ctx.data}\")  # Expected: [1.0, 2.0, 3.0]\n",
        "print(f\"Attempts: {ctx.attempt}\")  # Expected: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Composable Steps\n",
        "\n",
        "Steps can be nested. This example nests calibration and validation within a processing step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@pipestep(name=\"calibrate\", fingerprint_keys=(\"data\",))\n",
        "def calibrate(ctx: PipelineContext) -> PipelineContext:\n",
        "    ctx.calibrated = [x * 1.5 for x in ctx.data]  # Calibration factor\n",
        "    return ctx\n",
        "\n",
        "\n",
        "# Reuse validate from above\n",
        "process = Step(\"process\", children=[calibrate, validate])\n",
        "pipeline = Pipeline([load_data, process], name=\"nested_pipeline\")\n",
        "ctx = PipelineContext()\n",
        "pipeline.run(ctx)\n",
        "\n",
        "print(f\"Calibrated data: {ctx.calibrated}\")  # Expected: [1.5, 3.0, 4.5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Bio-Inspired Workflow (LBA)\n",
        "\n",
        "tinypipe was initially designed for ligand-binding assay (LBA) pipelines. This example mimics loading, calibrating, and validating assay data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@pipestep(name=\"load_assay_data\")\n",
        "def load_assay_data(ctx: PipelineContext) -> PipelineContext:\n",
        "    ctx.assay_data = [0.1, 0.2, 0.3]  # Simulated absorbance readings\n",
        "    return ctx\n",
        "\n",
        "\n",
        "@pipestep(name=\"calibrate_assay\", fingerprint_keys=(\"assay_data\",))\n",
        "def calibrate_assay(ctx: PipelineContext) -> PipelineContext:\n",
        "    # Apply calibration curve (simplified)\n",
        "    ctx.concentrations = [x * 100 for x in ctx.assay_data]  # Convert to ng/mL\n",
        "    return ctx\n",
        "\n",
        "\n",
        "@pipestep(name=\"validate_assay\", fingerprint_keys=(\"concentrations\",))\n",
        "def validate_assay(ctx: PipelineContext) -> PipelineContext:\n",
        "    if any(x < 0 for x in ctx.concentrations):\n",
        "        ctx.abort_pass()  # Retry if invalid\n",
        "    return ctx\n",
        "\n",
        "\n",
        "lba_process = Step(\"lba_process\", children=[calibrate_assay, validate_assay])\n",
        "pipeline = Pipeline([load_assay_data, lba_process], name=\"lba_pipeline\", max_passes=3)\n",
        "ctx = PipelineContext()\n",
        "pipeline.run(ctx)\n",
        "\n",
        "print(f\"Concentrations: {ctx.concentrations}\")  # Expected: [10.0, 20.0, 30.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Observability with step_meta\n",
        "\n",
        "The `step_meta` dictionary tracks timing, cache hits, and status for each step. Let's inspect it after running the LBA pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Step metadata:\")\n",
        "for step_name, meta in ctx.step_meta.items():\n",
        "    print(f\"{step_name}: {meta}\")\n",
        "    # Example: {'input_hash': '...', 'status': 'ok', 'duration': 0.001}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Error Handling and Pipeline Abort\n",
        "\n",
        "Steps can raise exceptions or use `ctx.abort_pipeline()` to stop execution. Here, we simulate a fatal error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@pipestep(name=\"faulty_step\")\n",
        "def faulty_step(ctx: PipelineContext) -> PipelineContext:\n",
        "    ctx.abort_pipeline()  # Stop the pipeline\n",
        "    return ctx\n",
        "\n",
        "\n",
        "pipeline = Pipeline([load_data, faulty_step], name=\"error_pipeline\")\n",
        "ctx = PipelineContext()\n",
        "pipeline.run(ctx)  # Logs: aborted after N pass(es)\n",
        "\n",
        "print(f\"Data after abort: {getattr(ctx, 'data', None)}\")  # Expected: [1.0, 2.0, 3.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "tinypipe provides a lightweight, type-safe way to build sequential pipelines with caching, nesting, and flexible control. Its `@pipestep` decorator simplifies step creation, while `PipelineContext` helpers (`abort_pass`, `abort_pipeline`) ensure robust flow control."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tinypipe--pG_C7gH-py3.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
